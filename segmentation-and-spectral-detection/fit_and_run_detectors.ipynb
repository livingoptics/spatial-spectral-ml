{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is subject to the terms and conditions defined in file\n",
    "# `COPYING.md`, which is part of this source code package.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import numpy.typing as npt\n",
    "from ultralytics.utils.plotting import Colors\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from classifiers.fastsam.model import FastSAM\n",
    "from classifiers.helpers import draw_detections, get_spectra_from_mask\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from lo_dataset_reader import DatasetReader, rle_to_mask\n",
    "from lo.sdk.analysis.ml.models.spectral_classifier import CPURFClassifier as classifier\n",
    "from lo.sdk.api.acquisition.data.formats import LORAWtoLOGRAY12, _debayer\n",
    "from lo.sdk.helpers.import_numpy_or_cupy import xp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastsam_url = \"https://github.com/ultralytics/assets/releases/download/v8.3.0/FastSAM-x.pt\"\n",
    "filename = \"FastSAM-x.pt\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    subprocess.run([\"wget\", fastsam_url, \"-O\", filename])\n",
    "else:\n",
    "    print(f\"{filename} already exists. Skipping download.\")\n",
    "\n",
    "yolo_url = \"https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l-seg.pt\"\n",
    "filename = \"yolov8l-seg.pt\"\n",
    "\n",
    "if not os.path.exists(filename) or os.path.getsize(filename) < 85 * 1024 * 1024:\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    subprocess.run([\"wget\", yolo_url, \"-O\", filename])\n",
    "else:\n",
    "    print(f\"{filename} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods\n",
    "\n",
    "def label_fn(ann):\n",
    "    \"\"\"\n",
    "    Generate a class label string from the metadata of an annotation\n",
    "    Args:\n",
    "        ann (lo.data.tools.tools.Annotation):\n",
    "\n",
    "    Returns:\n",
    "        label (str): the class label for the annotation\n",
    "    \"\"\"\n",
    "    return ann[\"category_name\"]\n",
    "\n",
    "def percentile_norm(im: np.ndarray, low: int = 1, high: int = 99) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalise the image based on percentile values.\n",
    "\n",
    "    Args:\n",
    "        im (xp.ndarray): The input image.\n",
    "        low (int): The lower percentile for normalization.\n",
    "        high (int): The higher percentile for normalization.\n",
    "\n",
    "    Returns:\n",
    "        xp.ndarray: The normalised image.\n",
    "    \"\"\"\n",
    "\n",
    "    ## ::30 is subsampling\n",
    "    low, high = np.percentile(im[::40, ::40], (low, high), axis=(0, 1))\n",
    "    im = (im - low) / (high - low)\n",
    "    return np.clip(im, 0, 1) * 255\n",
    "\n",
    "def get_true_values_from_mask(values: np.ndarray, sampling_coordinates: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select elements of values whose coordinates in sampling_coordinates map to True values in mask.\n",
    "    Args:\n",
    "        values (np.ndarray): An array of values with shape (n_spectra, ...)\n",
    "        sampling_coordinates (np.ndarray): An array of sampling coordinates with shape (n_spectra, 2)\n",
    "        mask (np.ndarray): a binary mask with shape (h, w)\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): an array of selected values with shape (n_selected, ...)\n",
    "    \"\"\"\n",
    "    sampling_coordinates = np.int32(sampling_coordinates)\n",
    "    in_mask = mask[sampling_coordinates[:, 0], sampling_coordinates[:, 1]]\n",
    "    return values[in_mask.astype(bool), :]\n",
    "\n",
    "def visualise(scene, pred):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        scene: Original image (H, W) or (H, W, 3)\n",
    "        pred: Dictionary mapping class IDs to coordinate arrays\n",
    "    Returns:\n",
    "        vis_img: RGB image with overlaid predictions\n",
    "    \"\"\"\n",
    "    # Create empty mask\n",
    "    h, w = scene.shape[:2]\n",
    "    \n",
    "    # Define color map (adjust according to your actual class labels)\n",
    "    color_map = {\n",
    "        0: (255, 255, 0),       # background (cyan)\n",
    "        1: (0, 255, 0),     # class 1 (green) - grapes\n",
    "        2: (255, 0, 0),     # class 2 (blue) - tray\n",
    "        3: (0, 0, 255),     # class 3 (red) - tyvec\n",
    "    }\n",
    "    \n",
    "    # Create a copy of the scene to draw on\n",
    "    vis_img = scene.copy()\n",
    "    \n",
    "    # Draw each class with its corresponding color\n",
    "    for class_id, coords in pred.items():            \n",
    "        # Ensure coordinates are integers and within bounds\n",
    "        coords = coords.astype(int)\n",
    "        valid_coords = (coords[:, 0] < h) & (coords[:, 1] < w) & (coords[:, 0] >= 0) & (coords[:, 1] >= 0)\n",
    "        coords = coords[valid_coords]\n",
    "        \n",
    "        # Draw each point as a small circle\n",
    "        for y, x in coords:\n",
    "            cv2.circle(vis_img, (x, y), radius=5, color=color_map[class_id], thickness=-1)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_height = 60\n",
    "    legend = np.zeros((legend_height, scene.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "    class_names = {\n",
    "        0: \"Background\",\n",
    "        1: \"Grapes\",\n",
    "        2: \"Tray\",\n",
    "        3: \"Tyvec\"\n",
    "    }\n",
    "\n",
    "    font_scale = 0.9\n",
    "    font_thickness = 2\n",
    "    box_width = 25\n",
    "    box_height = 25\n",
    "    y_box_top = 17\n",
    "    y_text = y_box_top + box_height - 5\n",
    "\n",
    "    x_pos = 10\n",
    "    spacing = 16\n",
    "    for class_id, color in color_map.items():\n",
    "        x_box = x_pos\n",
    "        x_text = x_box + box_width + spacing\n",
    "\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(legend, (x_box, y_box_top), (x_box + box_width, y_box_top + box_height), color, -1)\n",
    "\n",
    "        # Draw outlined text\n",
    "        text = class_names[class_id]\n",
    "        cv2.putText(legend, text, (x_text, y_text), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_scale, (0, 0, 0), font_thickness + 1, lineType=cv2.LINE_AA)\n",
    "        cv2.putText(legend, text, (x_text, y_text), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_scale, (255, 255, 255), font_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "        x_pos += box_width + spacing + cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0][0] + 40\n",
    "\n",
    "    # Combine image and legend\n",
    "    vis_img = np.vstack([vis_img, legend])\n",
    "\n",
    "    return vis_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset using the DatasetReader class\n",
    "# path = \"/path/to/dataset\"\n",
    "path = \"/home/seonghyun/hugging_face/Grapes-Dataset.zip\"\n",
    "reader = DatasetReader(dataset_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save classifier post training\n",
    "model_path = './fruit_classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the spectral classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectra = []\n",
    "all_labels = []\n",
    "\n",
    "class_number_to_label = {\n",
    "    0: \"background\",\n",
    "    1: \"grapes\",\n",
    "    2: \"tray\",\n",
    "    3: \"tyvec\",\n",
    "\n",
    "}\n",
    "label_to_class_number = {v: k for k, v in class_number_to_label.items()}\n",
    "\n",
    "for (info, scene_frame, spectra, *_), _, annotations, *_ in reader:\n",
    "    print(\"scene_frame shape:\", scene_frame.shape)\n",
    "    \n",
    "    h, w = scene_frame.shape[:2]\n",
    "    coordinates = info.sampling_coordinates.astype(int)  # shape: (N, 2)\n",
    "\n",
    "    # Create a label mask for the whole frame\n",
    "    label_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    for ann in annotations:\n",
    "        class_name = label_fn(ann)\n",
    "        print(class_name)\n",
    "        if class_name not in label_to_class_number:\n",
    "            continue\n",
    "\n",
    "        label_index = label_to_class_number[class_name]\n",
    "\n",
    "        if ann.get(\"segmentation\"):\n",
    "            mask = rle_to_mask(ann[\"segmentation\"], (h, w))\n",
    "            label_mask[(mask > 0) & (label_mask == 0)] = label_index\n",
    "\n",
    "    # Map each spectrum point to its label\n",
    "    for i, (y, x) in enumerate(coordinates):\n",
    "        if 0 <= y < h and 0 <= x < w:\n",
    "            label = label_mask[y, x]\n",
    "            if label in class_number_to_label:\n",
    "                spectrum = spectra[i]  # shape: (96,)\n",
    "                all_spectra.append(spectrum)\n",
    "                all_labels.append(label)\n",
    "\n",
    "# Convert to arrays\n",
    "all_spectra = np.stack(all_spectra)  # (N, 96)\n",
    "all_labels = np.array(all_labels)    # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Instantiate Classifier\n",
    "classifier = classifier(\n",
    "    classifier_path=model_path,\n",
    "    plot_spectra=False,\n",
    "    do_reflectance=False,\n",
    "    class_number_to_label=class_number_to_label,\n",
    ")\n",
    "\n",
    "print(\"Starting training\")\n",
    "classifier.train(\n",
    "    all_spectra=all_spectra,\n",
    "    all_labels=all_labels,\n",
    "    n_estimators=70,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "# Print class labels\n",
    "for k, v in classifier.metadata.items():\n",
    "    print(k, v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run segmentation enhanced with spectral classification\n",
    "\n",
    "This inference example demonstrates a simple integration between the YOLO SAM model and a spectral classifier to achieve subclass classification and improved recognition while preserving the semantic understanding of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained spectral classifier\n",
    "prob_thresh = 0.7\n",
    "\n",
    "# load segment anything model developed by ultralytics.\n",
    "model = FastSAM('FastSAM-x.pt') \n",
    "plt.figure()\n",
    "\n",
    "count = 0\n",
    "for (info, scene_frame, spectra, *_), *_ in reader:\n",
    "\n",
    "        if scene_frame is None:\n",
    "            break\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "        scene_frame = np.ascontiguousarray(scene_frame)\n",
    "\n",
    "        # Prepare the scene image\n",
    "        if len(scene_frame.shape) == 3:\n",
    "            scene_frame = scene_frame.squeeze()\n",
    "        if np.amax(scene_frame) > 1000:\n",
    "            scene_frame = LORAWtoLOGRAY12(scene_frame)\n",
    "\n",
    "        if scene_frame.shape[0] % 2 == 1 or scene_frame.shape[1] % 2 == 1:\n",
    "            scene_frame = np.dstack([scene_frame, scene_frame, scene_frame])\n",
    "        else:\n",
    "            scene_frame = _debayer(scene_frame)\n",
    "\n",
    "        results = model(scene_frame, device='cpu', retina_masks=True, imgsz=480, conf=0.6, iou=0.9)\n",
    "        masks = results[0].masks.data.detach().cpu().numpy()\n",
    "        segs = results[0].masks.xy\n",
    "        boxes = results[0].boxes.data.detach().cpu().numpy()\n",
    "\n",
    "        frame = (info, scene_frame, spectra)\n",
    "        metadata_out, classes, probs = classifier(\n",
    "            frame,\n",
    "            confidence=prob_thresh,\n",
    "            sa_factor=4,\n",
    "            similarity_filter=False,\n",
    "        )\n",
    "\n",
    "        vis = visualise(scene_frame, metadata_out)\n",
    "\n",
    "        save_dir = \"visualisations\"\n",
    "        save_path = os.path.join(save_dir, f\"output_{count:03d}.jpg\")\n",
    "        cv2.imwrite(save_path, vis)\n",
    "\n",
    "        # assign classes to objects identify by segment anything\n",
    "        for bbox, mask, seg in zip(boxes, masks, segs):\n",
    "            bb_probs = get_spectra_from_mask(probs, info.sampling_coordinates, mask)\n",
    "            \n",
    "            bb_class = np.argmax(np.median(bb_probs, 0))\n",
    "            if bb_class == 0:\n",
    "                continue\n",
    "            bb_prob = bb_probs[:,bb_class].mean(0)\n",
    "            if bb_prob < prob_thresh:\n",
    "                continue\n",
    "            scene_frame = draw_detections(classifier.classes, scene_frame, bbox, bb_prob, bb_class, seg)    \n",
    "\n",
    "        count += 1\n",
    "        plt.imshow(scene_frame)\n",
    "        plt.show()\n",
    "        time.sleep(0.2) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run vanila YOLO segmentation model:\n",
    "\n",
    "This inference example shows a visualisation of the performance of a pretrained YOLOv8, for performance comparison purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run model\n",
    "reader = DatasetReader(dataset_path=path)\n",
    "\n",
    "model = YOLO(\"yolov8l-seg.pt\")\n",
    "\n",
    "plt.figure()\n",
    "count = 0\n",
    "for (info, scene_frame, spectra, *_), *_ in reader:\n",
    "        if scene_frame is None:\n",
    "            break\n",
    "        \n",
    "        clear_output()\n",
    "        scene_frame = np.ascontiguousarray(scene_frame)\n",
    "\n",
    "        # Prepare the scene image\n",
    "        if len(scene_frame.shape) == 3:\n",
    "            scene_frame = scene_frame.squeeze()\n",
    "        if np.amax(scene_frame) > 1000:\n",
    "            scene_frame = LORAWtoLOGRAY12(scene_frame)\n",
    "\n",
    "        if scene_frame.shape[0] % 2 == 1 or scene_frame.shape[1] % 2 == 1:\n",
    "            scene_frame = np.dstack([scene_frame, scene_frame, scene_frame])\n",
    "        else:\n",
    "            scene_frame = _debayer(scene_frame)\n",
    "\n",
    "        results = model(scene_frame, device='cpu', retina_masks=True, imgsz=480, conf=0.05, iou=0.9)\n",
    "        \n",
    "        segments = results[0].masks.xy\n",
    "        boxes = results[0].boxes.data.detach().cpu().numpy()\n",
    "        for ((*box, conf, cls_), segment) in zip(boxes, segments):\n",
    "            scene_frame = draw_detections(classifier.classes, scene_frame, bbox, bb_prob, bb_class, seg)    \n",
    "\n",
    "        count += 1\n",
    "        plt.imshow(scene_frame)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
